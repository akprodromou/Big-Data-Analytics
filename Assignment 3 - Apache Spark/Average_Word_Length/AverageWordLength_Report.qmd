---
title: "Technologies for Big Data Analytics"
subtitle: "Εργασία 3 - Programming in Spark - Υποερώτημα 1: Average Word Length"

author: "Antonis Prodromou"

date: today

format:
  pdf:
    toc: true
    number-sections: true
    colorlinks: true
    lang: el-GR
    highlight-style: github

    mainfont: "Roboto"
    sansfont: "Arial"
    monofont: "Menlo"

    df-print: kable

    include-in-header:
      text: |
        % no polyglossia
        \usepackage{pdflscape}
    babel-lang: "none"
---

## Περιγραφή Λύσης - Υποερώτημα 1

### Σκοπός

Στην εργασία αυτή χρησιμοποιούμε την Resilient Distributed Dataset (RDD) API του Spark για να υπολογίσουμε τον μέσο όρο του μήκους των λέξεων που ξεκινούν από ένα συγκεκριμένο χαρακτήρα (a-z). Ως δεδομένα εισόδου χρησιμοποιούμε το βιβλίο «The Adventures of Sherlock Holmes» του Sir Arthur Conan Doyle, που διατίθεται από τη βιβλιοθήκη Gutenberg. Το πρόγραμμα έχει συνταχθεί ώστε να κάνει μια κανονικοποίηση του κειμένου, διαγράφοντας όλα τα σημεία στίξης και μετατρέποντας όλους τους χαρακτήρες σε πεζούς.

Η έξοδος του προγράμματος είναι τα αποτελέσματα ταξινομημένα με βάση το μέσο όρο, εμφανίζοντας πρώτα τα γράμματα με μεγαλύτερο μέσο όρο.

---

### Περιβάλλον

Τρέχουμε το πρόγραμμα στο Local File System (FS) σε Unbutu Virtual Machine. Ως εκτελεστή έχουμε τον driver (1 core), δηλαδή δεν έχουμε worker executors που θα είχαμε στην περίπτωση του cluster mode, κάτι το οποίο περνάμε ως configuration και στον κώδικα (βλ. παρακάτω). Τα αποτελέσματα της εκτέλεσης είναι διαθέσιμα τοπικά στο `http://10.0.2.15:4040` του VM.

---

### Κλάσεις

Η Scala διαφοροποιείται έναντι της Java αναφορικά με τον τρόπο δημιουργίας στατικών μεθόδων: ενώ στην Java οι μέθοδοι που ανήκουν στην κλάση και όχι στις επιμέρους αρχικοποιήσεις ορίζονται με τον όρο `static`, στη Scala αυτό επιτυγχάνεται με τη δημιουργία `object` αντί για `class`. Αυτό δίνεται και στο documentation της γλώσσας «Methods and values that aren’t associated with individual instances of a class belong in singleton objects, denoted by using the keyword object instead of class». Η στατική μέθοδος είναι απαιτούμενη ως σημείο εισόδου της εφαρμογής στη Scala, όπως και στη Java. Ούτως ή άλλως, η συγκεκριμένη εφαρμογή τρέχει μια φορά χωρίς να χρειάζονται αντίτυπά της ή διαφοροποιήσεις της σε άλλες αρχικοποιήσεις, που θα απαιτούσαν τη δημιουργία κλάσεων.

#### WordCount

Η `main` της WordCount ορίζεται να έχει ως παράμετρο (όταν την καλούμε στο command line) τη διύθυνση του αρχείου εισόδου (το κείμενο). Ξεκινάει με τη ρύθμιση των παραμέτρων του προγράμματος δημιουργώντας ένα `SparkConf()` αντικείμενο. Εκεί ορίζουμε πως το πρόγραμμα θα τρέξει τοπικά, με την παράμετρο `local[*]`. Το `SparkConf` εσωκλείεται στη συνέχεια σε αντικείμενο `SparkContext`, το οποίο αναπαριστά τη σύνδεση με μια συστοιχία υπολογιστών (cluster) πάνω στους οποίους θα τρέξει η εφαρμογή, που εν προκειμένω είναι μόνο ένας υπολογιστής, ο πυρήνας του Virtual Machine.

Η μέθοδος `textFile` του SparkContext διαβάζει το αρχείο κειμένου δημιουργώντας ένα Resilient Distributed Dataset (RDD), μετατρέποντας κάθε γραμμή του κειμένου σε μια συλλογή με επιμέρους στοιχεία. Τα στοιχεία αυτά ομαδοποιούνται σε partitions του RDD ανάλογα με το μέγεθος του αρχείου και τις ρυθμίσεις του `SparkConf` αυτόματα στην περίπτωσή μας· είναι όμως δυνατό να ορίσουμε τα partitions περνώντας τα ως δεύτερο όρισμα στη μέθοδο. Γνωρίζουμε από τη θεωρία πως στη Spark όλες οι ενέργειες που γίνονται έχουν να κάνουν είτε με δημιουργία νέων RDDs (π.χ.`textFile`), είτε με την μετατροπή υπαρχόντων RDDs (`map`), είτε με την εφαρμογή πράξεων σε ένα RDD για τον υπολογισμό ενός αποτελέσματος (π.χ. `reduce`).

Επομένως ξεκινάμε τους μετασχηματισμούς, οι οποίοι παίρνουν το RDD και κατά σειρά επεξεργαζόνται το κείμενο δημιουργώντας νέα RDD ως εξής:

- `.map(_.toLowerCase)`: το πρώτο transformation μετατρέπει το κείμενο σε πεζούς χαρακτήρες.
- `.map(_.replaceAll("[^a-z0-9\\s]", " "))`: βρίσκει όλα τα σύμβολα, δηλαδή οποιονδήποτε χαρακτήρα δεν είναι πεζό γράμμα (`a-z`), αριθμός (`0-9`) ή κενό-whitespace  και τον αντικαθιστά με κενό.
- `.flatMap(_.split("\\s+"))`: δημιουργεί μια ακολουθία χωρίζοντας το κείμενο με βάση τα whitespaces (`\s`) που βρίσκει.
- `.filter(_.nonEmpty)`: κρατάει όσα στοιχεία της ακολουθίας δεν είναι άδεια.
- `.filter(!_.charAt(0).isDigit)`: αγνοεί τις λέξεις που ξεκινούν με αριθμό.

Το RDD μας αυτή τη στιγμή έχει τη μορφή `RDD[String]`, δηλαδή μεμονωμένες λέξεις. Με τον επόμενο μετασχηματισμό, το μετατρέπουμε στη μορφή `RDD[(Key, Value)]`:

- `.map(word => (word.charAt(0), (word.length, 1)))` με το value να είναι ένα tuple (μήκος λέξης, 1), δηλαδή απαριθμούμε και την εμφάνιση της κάθε λέξης, με σκοπό να τις αθροίσουμε στη συνέχεια.

H `reduceByKey` συνδυάζει όλα τα values που έχουν το ίδιο κλειδί και τους εφαρμόζει τη συνάρτηση που δέχεται σαν όρισμα. Εδώ λέμε στο πρόγραμμα πως όταν βρει κοινό κλειδί μεταξύ 2 στοιχείων του RDD, να αθροίσει τα πρώτα στοιχεία του Value μεταξύ τους (`a._1 + b._1`), και τα δεύτερα αντίστοιχα (`a._2 + b._2`):

`.reduceByKey((a, b) => (a._1 + b._1, a._2 + b._2))`

Με το RDD να γίνεται: RDD[(K, (Total Length of individual Word, Total Occurence Count of Word))]. Επομένως κάνουμε και τη διαίρεση μεταξύ των στοιχείων του Value για να πάρουμε τον μέσο όρο:

```
.mapValues {
  case(totalLen, totalCount) =>
    (totalLen.toDouble / totalCount).formatted("%.2f").toDouble
}
```

Το `case` εδώ είναι ένας ιδιωματισμός της Scala που επιτελεί ό,τι και τα a._1, b._1 νωρίτερα, όμως κάνει τον κώδικα πιο κατανοητό. Τέλος ταξινομούμε με βάση το μέσο όρο (`.sortBy(_._2, ascending = false)`)

---

### Αποτελέσματα

Τα πρώτα 5 αποτελέσματα δίνονται παρακάτω, με τα υπόλοιπα διαθέσιμα στο επισυναπτόμενο αρχείο `part-00000.txt`:

```
(c,7.09)
(e,7.03)
(q,6.96)
(p,6.92)
(r,6.75)
```

Δηλαδή οι λέξεις που ξεκινούν από το γράμμα `c` έχουν το μεγαλύτερο μ.ό. μήκους με 7.09 χαρακτήρες.

---

### Spark Web UI

Μπορούμε να δούμε την παραπάνω διαδικασία και σε Directed Acyclic Graph (DAG) μέσα στο Web User Interface της Spark (στο `http://10.0.2.15:4040`):

![Directed Acyclic Graph (DAG)](results/DAG.png)

Ενώ (ενδεικτικά) για το Stage 0 που απεικονίζεται στο DAG, ο πίνακας των μετρικών είναι:

![Summary Metrics for Stage 0](results/summary_metrics.png)

Οπότε έχουμε 1 μόνο task το οποίο διήρκησε 0.9 δευτερόλεπτα. Το Garbage Collection (GC) είναι το overehead για την απελευθέρωση της μνήμης την οποία καταναλώνουν τα αντικείμενα που το πρόγραμμα δεν χρησιμοποιεί πια (15ms).

Επιπλέον είχαμε στην είσοδο 121706 στοιχεία στη συλλογή (γραμμές κειμένου δηλαδή) και στην έξοδο 26 (τα γράμματα της αγγλικής αλφαβήτου).


---

